{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd17876",
   "metadata": {},
   "source": [
    "# Part 2 (21 Marks)\n",
    "\n",
    "## The Task\n",
    "Pick one of the interactions from Part 1 and implement it, using audio input and output on\n",
    "your computer. Instead of putting you into a telephone queue (or doing some other action),\n",
    "the program will just print out the queue/action it would have done.\n",
    "There is a step-by-step breakdown and marking rubric later in this document.\n",
    "\n",
    "## Format\n",
    "Submit your git repository via github classroom. You can either submit a Jupyter notebook,\n",
    "or you can submit a program. Writing it as a program is probably more fun though. You\n",
    "don't need to include any models you have downloaded in the git repo if the program can\n",
    "download them from the internet.\n",
    "\n",
    "## Use of ChatGPT\n",
    "Absolutely yes, do use any large language model you like to help you code this faster, and\n",
    "help you out when things don't work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44956ac8",
   "metadata": {},
   "source": [
    "# Step-by-step breakdown\n",
    "\n",
    "1. Start by writing a program that can play audio through the speakers (or headphones). Test that the system audio is enabled. There are many dfferent audio file formats (MP3, WAV, AU); if you need to, ask ChatGPT for ways to load these files into a format that your program can use. I found the __sounddevice__ library easy to set up and use. If you get stuck, just print something out with print() and use it as a placeholder.\n",
    "\n",
    "2. Then write a program that can record 3 seconds of audio. I found it helpful to print something out to mark the time that it started recording, and then print something out 3 seconds later. If you get stuck, record some audio samples on your phone and copy them on to your computer.\n",
    "\n",
    "3. Write a program that takes an audio  le and transcribes it. This is a handy little program to have around anyway; I write a lot of stu  using speech recognition nowadays. If you get stuck, you can use input() and have someone type the transcript in instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ab022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As mentioned in part 1, we need to install this library. \n",
    "# To work with audio files in Python, you can use libraries like sounddevice, pydub, or speech_recognition. \n",
    "#!pip install sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d61f81fa-190d-446b-9fb7-a1d7df162435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homebrew 4.2.11\n",
      "Python 3.10.13\n",
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "!brew --version\n",
    "!python --version\n",
    "#!keras --version\n",
    "#!transformers --version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1c879e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import sounddevice as sd\n",
    "import scipy.io.wavfile as wavfile\n",
    "import soundfile as sf\n",
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# LLM Libraries\n",
    "from transformers import pipeline\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cda481eb-242a-4bfc-8ef4-1e354b3525ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path  = \"audio2.wav\"\n",
    "recorded_audio = \"recorded_audio.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccd967b1-2060-4c9a-a944-b658265fe5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1: Play Audio\n",
    "\n",
    "def play_audio(file_name):\n",
    "    # Load data\n",
    "    fs, data = wavfile.read(file_name)\n",
    "\n",
    "    # Play data\n",
    "    sd.play(data, fs)\n",
    "    status = sd.wait()\n",
    "\n",
    "play_audio(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f1c85f0-e06f-459e-a794-fdce70992e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 2: Record Audio\n",
    "\n",
    "def record_audio():\n",
    "    # Define the duration of the recording in seconds\n",
    "    duration = 3  # seconds\n",
    "\n",
    "    # Define the sampling frequency\n",
    "    fs = 44100  # Hz\n",
    "\n",
    "    # Start recorder with the given sampling frequency and channels as input\n",
    "    # The default input device will be used\n",
    "    print(\"Recording Audio Now...\")\n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')\n",
    "\n",
    "    # Record audio for the given number of frames\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "\n",
    "    # Stop recorder\n",
    "    sd.stop()\n",
    "\n",
    "    print(\"Audio Recording Complete!\")\n",
    "\n",
    "    # Save the recording to a .wav file\n",
    "    sf.write('recorded_audio.wav', recording, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58279847-df7c-40af-8772-573f28747266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording Audio Now...\n",
      "Audio Recording Complete!\n"
     ]
    }
   ],
   "source": [
    "record_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61bb466c-d9aa-47a0-b18c-3feacfc56e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the plau_audio function to play the recorded audio\n",
    "play_audio(recorded_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17ba1a99-c25c-4f00-9a8d-361a6a6a41d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 3: Audio to text\n",
    "\n",
    "def audio_to_text(file_name):\n",
    "    # Create a recognizer object\n",
    "    r = sr.Recognizer()\n",
    "\n",
    "    # Load an audio file\n",
    "    with sr.AudioFile(file_name) as f:\n",
    "        audio_data = r.record(f)\n",
    "\n",
    "    # Transcribe the audio data\n",
    "    text = r.recognize_google(audio_data)\n",
    "    print(\"Transcription: \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8bfa99e-b80c-4abd-a1f3-754e7b5b861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech recognition with Whisper pipeline\n",
    "\n",
    "transcriber = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0add220-8532-4e4f-a6b8-2be6517a3ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 4: Speech recognition with Whisper\n",
    "\n",
    "def speech_recognition(file_name):\n",
    "    result = transcriber(file_name)\n",
    "    #print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8785f1e5-666d-4cdb-8ef4-d8b923b9e61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  hello how's it going\n"
     ]
    }
   ],
   "source": [
    "audio_to_text(recorded_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9162779b-e620-4243-8a4e-1e5a0f33ffd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' I want to cancel my service.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_recognition(recorded_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e09446-1bab-444f-83b8-caf0b54fc2d5",
   "metadata": {},
   "source": [
    "# Step-by-step Breakdown\n",
    "\n",
    "4. Record some audio files in your best sounding voice (or get someone else to do it, find a text-to-speech system or find some samples on the internet). You'll need a greeting, and a confirmation message for each dfferent intent.\n",
    "\n",
    "5. Play around with a few different (local) generative language models. This will involve a lot of downloading and waiting, so do this somewhere where you have a lot of band-width. (Or when you have a lot of chores to do.) Come up with a prompt that includes some examples and make sure that it works with your intents. Once you have that figured out, turn it into a program that prints out what the language model returned.\n",
    "\n",
    "6. Now modify that program so that you can identify the intent that is being mentioned in the output. Maybe you will look for a keyword being used. You can see whether the model has a JSON mode. That might make it easier to process the model's output. Remember that these generative language models aren't always super-reliable, so don't stress out if you can't get something to work 100% of the time. 80% is good enough for this project. Also, don't stress out if it takes a long time to run. It's easier for debugging if you can make it faster (you could see if there's a streaming mode... maybe\r",
    "you can stop processing after a few tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00126036-e3a9-4c04-8499-7dfb305fdbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with Apple clang version 15.0.0 (clang-1500.1.0.2.5)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/6.1.1_4 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopenvino --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "[mp3 @ 0x13e706230] Estimating duration from bitrate, this may be inaccurate\n",
      "Input #0, mp3, from 'temp.mp3':\n",
      "  Duration: 00:00:05.98, start: 0.000000, bitrate: 32 kb/s\n",
      "  Stream #0:0: Audio: mp3, 24000 Hz, mono, fltp, 32 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mp3 (mp3float) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'greeting.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf60.16.100\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 24000 Hz, mono, s16, 384 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 pcm_s16le\n",
      "[out#0/wav @ 0x600001878840] video:0kB audio:280kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.027192%\n",
      "size=     280kB time=00:00:05.95 bitrate= 385.7kbits/s speed= 585x    \n"
     ]
    }
   ],
   "source": [
    "# Program 4: Text to speech\n",
    "# I am using (Google Text-to-Speech) library in Python to generate text-to-speech audio files. \n",
    "\n",
    "# Set up the text to be converted to speech\n",
    "text = \"Welcome to ABC telecommunication services, how may I help you today?\"\n",
    "\n",
    "# Create a gTTS object and set the language and slow parameter\n",
    "speech = gTTS(text=text, lang='en-us', tld ='co.in', slow=False) #tld ='co.in'\n",
    "\n",
    "# Save the audio data to a temporary file\n",
    "speech.save(\"temp.mp3\")\n",
    "\n",
    "# Convert the temporary file to a .wav file using ffmpeg\n",
    "os.system(\"ffmpeg -y -i temp.mp3 -f wav -acodec pcm_s16le greeting.wav\")\n",
    "\n",
    "# Remove the temporary file\n",
    "os.remove(\"temp.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fa62426-8378-435c-9ea8-46c956c2214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 5: text to speech \n",
    "\n",
    "def text_to_speech(text):\n",
    "    \"\"\"\n",
    "    Converts the given text to speech, saving the audio as a WAV file.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text to convert to speech.\n",
    "\n",
    "    The function saves the generated audio file as 'assistant_reply.wav'.\n",
    "    \"\"\"\n",
    "    # Create a gTTS object\n",
    "    speech = gTTS(text=text, lang='en', tld='co.in', slow=False)\n",
    "\n",
    "    # Save the audio data to a temporary MP3 file\n",
    "    temp_file = \"temp.mp3\"\n",
    "    speech.save(temp_file)\n",
    "\n",
    "    # Convert the temporary MP3 file to a WAV file using ffmpeg\n",
    "    wav_file = \"assistant_reply.wav\"\n",
    "    os.system(f\"ffmpeg -y -i {temp_file} -acodec pcm_s16le -ar 44100 -ac 1 {wav_file}\")\n",
    "\n",
    "    # Remove the temporary MP3 file\n",
    "    os.remove(temp_file)\n",
    "\n",
    "    print(\"Speech conversion completed and saved as 'assistant_reply.wav'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4db45bf9-4816-4f3d-b5d9-74132d738671",
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting = \"greeting.wav\"\n",
    "\n",
    "play_audio(greeting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c99b3d8e-b5f5-4825-bff4-9c2e6f1dedeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program 5: LLMs for Identifying Intent (Intent Recognition)\n",
    "\n",
    "# Initialize the text classification pipeline\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09a50020-7763-4069-8cfc-2891e4278f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 6: Classify Intent and Respond\n",
    "\n",
    "def identify_intent_and_respond(transcribed_text):\n",
    "    # Classify the intent of the transcribed text\n",
    "    intent_prediction = classifier(transcribed_text)[0]\n",
    "    print(f\"Detected intent: {intent_prediction['label']} with confidence {intent_prediction['score']}\")\n",
    "\n",
    "    # Map the predicted intent to a predefined response\n",
    "    # This is a simplified approach; you might need a more complex mapping strategy\n",
    "    for intent, response in intents_responses.items():\n",
    "        if intent in transcribed_text.lower():\n",
    "            return response\n",
    "    return \"I'm sorry, I didn't understand that. Can you please repeat?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6da0d-63a3-4f09-9b1c-96b9b24a8b87",
   "metadata": {},
   "source": [
    "## Zero shot learning with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf7f8a-3821-4a37-a2e4-de843e00f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and tokenizer\n",
    "# Note: Replace \"gpt-3\" with the actual model identifier you're using, such as \"EleutherAI/gpt-neo-2.7B\" or another suitable model\n",
    "#model_name = \"EleutherAI/gpt-neo-2.7B\" # This is a placeholder. Replace with the model you intend to use.\n",
    "#generator = pipeline('text-generation', model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8f3ab2f-134b-41ac-969a-cbe9f4a91e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are all examples of customer service responses using natural language processing (NLP) for customer service chatbots. \n",
      "\n",
      "**Here's a breakdown of the responses:**\n",
      "\n",
      "1. **\"Sure, I can help with that. I'll need to verify your account details first. Is that alright with you?\"** This is a clear and straightforward request for account verification.\n",
      "\n",
      "\n",
      "2. **\"Absolutely, I'll take a look into your recent billing history for any charges. Is that correct?\"** This shows empathy and attention to customer needs.\n",
      "\n",
      "\n",
      "3. **\"I need to update my payment method. Can you help me with that?\"** This expresses a specific need for assistance with payment methods.\n",
      "\n",
      "\n",
      "4. **\"I am sorry to hear that you want to cancel the service. I would now connect you to our team member for further assistance. I hope thats ok with you.\"** This handles a cancellation request with empathy and offers further assistance.\n",
      "\n",
      "\n",
      "5. **\"I want to inquire about new plans being offered.I want to inquire about new plans being offered.\"** This is an open-ended question about new plans, indicating a desire for information.\n",
      "\n",
      "\n",
      "These responses demonstrate the use of NLP to understand customer intent, provide relevant responses, and offer assistance in a conversational and engaging way.\n"
     ]
    }
   ],
   "source": [
    "# Create a model function\n",
    "\n",
    "intent1 = \"I want to inquire about new plans being offered.\"\n",
    "intent2 = \"I am not satisified with my service, I want to cancel it.\"\n",
    "intent3 = \"blah blah blahhhhhh blah!\"\n",
    "\n",
    "def ollama_model(text):\n",
    "    # Construct the few-shot learning prompt\n",
    "    prompt = \"\"\"\n",
    "    1. \n",
    "    Customer: I want to check my balance.\n",
    "    Assistant: Sure, I can help with that. I'll need to verify your account details first. Is that alright with you?\n",
    "\n",
    "    2. \n",
    "    Customer: Can you help me with my recent charges?\n",
    "    Assistant: Absolutely, I'll take a look into your recent billing history for any charges. Is that correct?\n",
    "\n",
    "    3. \n",
    "    Customer: I need to update my payment method.\n",
    "    Assistant: No problem. I'll direct you to our secure payment update portal. Is that what you want me to do?\n",
    "\n",
    "    4. \n",
    "    Customer: I want to cancel the service.\n",
    "    Assistant: I am sorry to hear that you want to cancel our service. I would now connect you to our team member for further assistance. I hope thats ok with you.\n",
    "\n",
    "    Customer: {}\"\"\".format(text)  # Assuming text is the input from the customer\n",
    "\n",
    "    prompt += \"\\n Based on the above conversations select the main category that prompt is based on\"\n",
    "\n",
    "    #Initiate the model\n",
    "    response = ollama.generate(model='gemma:2b', prompt=prompt + text)['response']\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    # def extract_response_for_intent(response_dict):\n",
    "    #     # Extract the full response string from the dictionary\n",
    "    #     full_response_text = response_dict['response']\n",
    "    \n",
    "    #     # Assuming each response starts with a number and a period as shown in your output\n",
    "    #     # We split the full response text into individual responses\n",
    "    #     individual_responses = full_response_text.split(\"\\n\\n\")\n",
    "    \n",
    "    #     # The latest response of interest might be the last in the list\n",
    "    #     # This depends on how the responses are organized; you might need to adjust\n",
    "    #     latest_response = individual_responses[-1]\n",
    "    \n",
    "    #     # Further processing to clean or isolate the part of the response you need\n",
    "    #     # For example, if you need to remove certain known leading characters or phrases, you can do so here\n",
    "    \n",
    "    #     return latest_response\n",
    "\n",
    "    # # Using the function\n",
    "    # response_text = extract_response_for_intent(response)\n",
    "    # print(\"Extracted Response:\", response_text)\n",
    "\n",
    "\n",
    "    # Assuming 'response' is a dictionary that contains the generated text\n",
    "    # You'll need to adjust the following line based on the actual structure of 'response'\n",
    "#    response_text = response['generated_text'].split('Assistant:')[-1].strip()\n",
    "\n",
    "#    print(\"Generated Response:\", response_text)\n",
    "#    return response_text  # To use this text in further steps\n",
    "\n",
    "    # Generate a response\n",
    "    #responses = generator(prompt, max_length=200, num_return_sequences=1)\n",
    "    #response_text = responses[0]['generated_text'].split('Assistant:')[-1].strip()\n",
    "\n",
    "    #print(\"Generated Response:\", response_text)\n",
    "\n",
    "ollama_model(intent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2efd6b6-3ceb-4263-a8f3-6b578e416872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8c14f3f-a95c-4a13-a6cd-ad48f6aaf41b",
   "metadata": {},
   "source": [
    "## Few shot learning on DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80e5a51d-5773-4642-82ba-db0e4411e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "\n",
    "dataset = [\n",
    "    {\"text\": \"How do I pay my bill?\", \"intent\": \"billing\"},\n",
    "    {\"text\": \"I want to see my latest charges.\", \"intent\": \"billing\"},\n",
    "    {\"text\": \"Is there an issue with the network?\", \"intent\": \"technical support\"},\n",
    "    {\"text\": \"My internet is very slow.\", \"intent\": \"technical support\"},\n",
    "    {\"text\": \"I would like to buy more data for my plan.\", \"intent\": \"purchase data plan\"},\n",
    "    {\"text\": \"How can I upgrade my current data package?\", \"intent\": \"purchase data plan\"},\n",
    "]\n",
    "\n",
    "# Convert intents to numerical labels\n",
    "intent_to_label = {intent: i for i, intent in enumerate(set([item['intent'] for item in dataset]))}\n",
    "labels = [intent_to_label[item['intent']] for item in dataset]\n",
    "texts = [item['text'] for item in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f946713-76ca-4dab-9f4c-37597c5ec13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation and dataset creation\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the texts\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for text in texts:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,                      # Sentence to encode\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64, # Pad & truncate all sentences\n",
    "                        padding = 'max_length',\n",
    "                        truncation = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks\n",
    "                        return_tensors = 'tf',     # Return TensorFlow tensors\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = np.concatenate(input_ids, axis=0)\n",
    "attention_masks = np.concatenate(attention_masks, axis=0)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Convert labels to one-hot\n",
    "labels = to_categorical(labels, num_classes=len(intent_to_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a269c50-8a70-49ee-a060-cf56289af896",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFDistilBertForSequenceClassification\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTFDistilBertForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdistilbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#, num_labels=len(intent_to_label)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Since we're using one-hot labels\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:2908\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2906\u001b[0m         model\u001b[38;5;241m.\u001b[39mbuild()  \u001b[38;5;66;03m# build the network with dummy inputs\u001b[39;00m\n\u001b[1;32m   2907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2908\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# build the network with dummy inputs\u001b[39;00m\n\u001b[1;32m   2910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m safetensors_from_pt:\n\u001b[1;32m   2911\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_pytorch_state_dict_in_tf2_model\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.10/site-packages/keras/src/layers/layer.py:224\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(original_build_method)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(obj\u001b[38;5;241m.\u001b[39mname, caller\u001b[38;5;241m=\u001b[39mobj):\n\u001b[0;32m--> 224\u001b[0m         \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1131\u001b[0m, in \u001b[0;36mTFPreTrainedModel.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1130\u001b[0m     call_context \u001b[38;5;241m=\u001b[39m get_call_context_function()\n\u001b[0;32m-> 1131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mcall_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39min_call:\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the model\n",
    "\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased') #, num_labels=len(intent_to_label)\n",
    "\n",
    "optimizer = Adam(learning_rate=5e-5)\n",
    "loss = 'categorical_crossentropy' # Since we're using one-hot labels\n",
    "metric = 'accuracy'\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Prepare the data as TensorFlow dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": input_ids, \"attention_mask\": attention_masks}, labels))\n",
    "train_dataset = train_dataset.batch(8) # You can adjust the batch size\n",
    "\n",
    "# Fine-tune the model\n",
    "model.fit(train_dataset, epochs=3) # Adjust the epochs as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5588160a-db83-4101-ad66-22668f07cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "model.save_pretrained('./my_intent_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33033917-17c0-4a10-a7a6-a06d88a481cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "\n",
    "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "model_path = './my_intent_model'\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec4a56d-d5d4-4823-9d13-97428986a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified Intent Recognition Function\n",
    "\n",
    "def identify_intent_and_respond(model, tokenizer, text):\n",
    "    # Tokenize the text\n",
    "    encoded_input = tokenizer(text, return_tensors='tf', padding=True, truncation=True, max_length=64)\n",
    "    \n",
    "    # Predict\n",
    "    output = model(encoded_input)\n",
    "    prediction = tf.nn.softmax(output.logits, axis=-1)\n",
    "    predicted_intent_id = tf.argmax(prediction, axis=1).numpy()[0]  # Assuming batch_size=1 for simplicity\n",
    "\n",
    "    # Map predicted intent ID back to intent label\n",
    "    label_to_intent = {value: key for key, value in intent_to_label.items()}\n",
    "    predicted_intent = label_to_intent[predicted_intent_id]\n",
    "\n",
    "    # Define responses for each intent\n",
    "    intents_responses = {\n",
    "        \"billing\": \"It sounds like you have a question about your bill. I'll connect you with our billing department.\",\n",
    "        \"technical support\": \"It seems you're experiencing technical issues. Let me put you through to technical support.\",\n",
    "        \"purchase data plan\": \"You'd like to purchase a data plan. I'll direct you to our sales team for assistance.\"\n",
    "    }\n",
    "\n",
    "    # Fetch the appropriate response\n",
    "    response = intents_responses.get(predicted_intent, \"I'm sorry, I didn't understand that. Can you please repeat?\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d2dd02-5bfd-4587-a69f-689db13765ad",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "Now, integrate these components into your IVR system flow:\n",
    "\n",
    "Play the greeting message.\n",
    "\n",
    "1. Record the customer's speech.\n",
    "2. Transcribe the speech to text.\n",
    "3. Identify the intent and generate a response.\n",
    "4. Play or use text-to-speech to communicate the response back to the customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c41273b9-7705-4d51-bdab-b7c6bd18de23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording Audio Now...\n",
      "Audio Recording Complete!\n",
      "Transcribed Text:  you\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'generated_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     play_audio(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_reply.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Finally, call the function to run the IVR system\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mrun_ivr_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 15\u001b[0m, in \u001b[0;36mrun_ivr_system\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranscribed Text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranscribed_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Step 4: Identify the intent and generate response using ollama\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mollama_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscribed_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Step 5: Use TTS to communicate the response \u001b[39;00m\n\u001b[1;32m     18\u001b[0m response_audio \u001b[38;5;241m=\u001b[39m text_to_speech(response_text)  \u001b[38;5;66;03m# This is a placeholder for a text-to-speech function\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[41], line 24\u001b[0m, in \u001b[0;36mollama_model\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     20\u001b[0m response \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mgenerate(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemma:2b\u001b[39m\u001b[38;5;124m'\u001b[39m, prompt\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Assuming 'response' is a dictionary that contains the generated text\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# You'll need to adjust the following line based on the actual structure of 'response'\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m response_text \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenerated_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAssistant:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response_text)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response_text\n",
      "\u001b[0;31mKeyError\u001b[0m: 'generated_text'"
     ]
    }
   ],
   "source": [
    "# final function which combines all\n",
    "\n",
    "def run_ivr_system():\n",
    "    # Step 1: Play greeting\n",
    "    play_audio(\"greeting.wav\")\n",
    "    \n",
    "    # Step 2: Record the customer's request\n",
    "    record_audio()\n",
    "    \n",
    "    # Step 3: Transcribe the speech\n",
    "    transcribed_text = speech_recognition(\"recorded_audio.wav\")['text']\n",
    "    print(f\"Transcribed Text: {transcribed_text}\")\n",
    "\n",
    "    # Step 4: Identify the intent and generate response using ollama\n",
    "    ollama_model(transcribed_text)\n",
    "\n",
    "    # Step 5: Use TTS to communicate the response \n",
    "    response_audio = text_to_speech(response_text)  # This is a placeholder for a text-to-speech function\n",
    "    play_audio(\"assistant_reply.wav\")\n",
    "\n",
    "# Finally, call the function to run the IVR system\n",
    "run_ivr_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df3b7e-8c1a-4fed-9ed1-85f48a4b3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Step 4: Identify the intent and generate response\n",
    "    #response = identify_intent_and_respond(model, tokenizer, transcribed_text)\n",
    "    #print(f\"Assistant Response: {response}\")\n",
    "    \n",
    "    # Step 4: Identify the intent and select a response\n",
    "    #response = identify_intent_and_respond(transcribed_text)\n",
    "    #print(f\"Assistant Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f330fc-32bb-4217-8ce9-4fd00b695888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
